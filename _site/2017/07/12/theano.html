<h1 id="theano">theano</h1>

<h2 id="基本用法">基本用法</h2>

<p>在 theano 中学会定义矩阵 matrix 和功能 function 是一个比较重要的事, 我们在这里简单的提及了一下在 theano 将要运用到的东西.</p>

<p>theano 和 tensorflow 类似，都是基于建立神经网络每个组件，在组件联系起来，数据放入组件，得到结果。</p>

<p>首先, 我们这次需要加载 theano 和 numpy 两个模块, 并且使用 theano 来创建 function.</p>

<p>import numpy as np
import theano.tensor as T
from theano import function
定义X和Y两个常量 (scalar)，把结构建立好之后，把结构放在function，在把数据放在function。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># basic</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dscalar</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>  <span class="c"># 建立 x 的容器</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dscalar</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>  <span class="c"># 建立 y 的容器</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span>     <span class="c">#  建立方程</span>

<span class="c"># 使用 function 定义 theano 的方程,</span>
<span class="c"># 将输入值 x, y 放在 [] 里,  输出值 z 放在后面</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">function</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">z</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>  <span class="c"># 将确切的 x, y 值放入方程中</span>
<span class="c"># 5.0</span>
</code></pre></div></div>

<p>使用 theano 中 的 <code class="highlighter-rouge">pp (pretty-print)</code> 能够打印出原始方程:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">theano</span> <span class="kn">import</span> <span class="n">pp</span>
<span class="k">print</span><span class="p">(</span><span class="n">pp</span><span class="p">(</span><span class="n">z</span><span class="p">))</span> 
<span class="c"># (x + y)</span>
</code></pre></div></div>

<p>定义矩阵，以及利用矩阵做相关运算:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>  <span class="c"># 矩阵 x 的容器</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>  <span class="c"># 矩阵 y 的容器</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>   <span class="c"># 定义矩阵加法</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">function</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">z</span><span class="p">)</span> <span class="c"># 定义方程</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> 
        <span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
        <span class="p">)</span>
      <span class="p">)</span>
<span class="s">"""
[[ 10.  11.  12.  13.]
 [ 14.  15.  16.  17.]
 [ 18.  19.  20.  21.]]
"""</span>
</code></pre></div></div>

<h2 id="搭建神经网络的一层">搭建神经网络的一层</h2>

<p>例如，以下的两行代码，就表示了一个具有两层神经元的神经网络：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># to define the layer like this:</span>
<span class="n">l1</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
<span class="n">l2</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">l1</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>其中，第一层网络我们命名为l1, 输入变量为inputs, 输入为1维，输出为10维，也就是说l, 含有10个神经元或节点； 激活函数为theano.tensor.nnet.relu函数, 当然我们也可以针对不同的问题选用别的函数, 例如theano.tensor.nnet.nnet.sigmoid。</p>

<p>第二层网络的输入是第一层网络的输出l1.outputs, 所以输入的维度是10，输出是1维，激活函数我们采用one,也就是说我们采用默认的线形激活函数， l2层含有1个神经元，也就是网络的输出神经元。</p>

<p>以上的代码，描述并构建了一个1-10-1的神经网络（inputs-l1-l2）。</p>

<p>接下来我们来具体的实现Layer类的代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">out_size</span><span class="p">,</span> <span class="p">))</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Wx_plus_b</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
        <span class="k">if</span> <span class="n">activation_function</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wx_plus_b</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Wx_plus_b</span><span class="p">)</span>
</code></pre></div></div>

<p>这段代码中，我们最关心的就是这个类的构造函数</p>

<p><code class="highlighter-rouge">def __init__(self, inputs, in_size, out_size, activation_function=None)</code>
和之前的例子一致，我们采用了相同的输入变量名。</p>

<p>接着，我们定义了W,b来代表该神经网络层的输入权值和偏置值，我们把W初始化为 由符合均值为0， 方差为1高斯分布的随机变量值组成的in_size-by-out_size的矩阵; b初始化为值为0.1的out_put-by-1的向量。 (当然，我们也可以采用不同的初始化方法，这里我们暂时不讨论初始化权值对最终神经网络训练的影响)。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">)))</span>
<span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">out_size</span><span class="p">,</span> <span class="p">))</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<p>首先我们要计算所有神经元的输入矩阵, 也就是输入inputs与输入权值W的点乘（dot product）在加上偏置值b：</p>

<p><code class="highlighter-rouge">self.Wx_plus_b = T.dot(inputs, self.W) + self.b</code></p>

<p>然后，我们需要根据我们构造神经层指定的激活函数类型activation_function,来计算神经层的输出向量。 这里我们假设如果activation_function是None， 那就是该层神经元采用线形输出；如果是其他Theano的激活函数，就把Wx_plus_b作为该层激活函数的输入，同时函数的输出即为神经层的输出：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
<span class="k">if</span> <span class="n">activation_function</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wx_plus_b</span>
<span class="k">else</span><span class="p">:</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Wx_plus_b</span><span class="p">)</span>
</code></pre></div></div>

<p>我们就成功的定义了神经网络的最最重要的结构–神经层Layer。</p>

<h2 id="创建分类神经网络模型">创建分类神经网络模型</h2>

<h4 id="导入模块并创建数据">导入模块并创建数据</h4>

<p>引入需要使用的Python包：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="n">T</span>
</code></pre></div></div>

<p>先定义一个功能，用来计算分类问题的准确率，即预测的类别中有多少是和实际类别一样的，计算出百分比。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_accuracy</span><span class="p">(</span><span class="n">y_target</span><span class="p">,</span> <span class="n">y_predict</span><span class="p">):</span>
    <span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y_predict</span><span class="p">,</span> <span class="n">y_target</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy</span>
</code></pre></div></div>

<p>用 <code class="highlighter-rouge">randn</code> 随机生成数据集。 D 中的 <code class="highlighter-rouge">input_values</code> 是 400 个样本，784 个<code class="highlighter-rouge">feature</code>。 <code class="highlighter-rouge">target_class</code> 是有两类，0 和 1。 要做的是，用神经网络训练数据学习哪些输入对应 0，哪些对应 1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">400</span>             <span class="c"># training 数据个数</span>
<span class="n">feats</span> <span class="o">=</span> <span class="mi">784</span>         <span class="c"># input 的 feature 数</span>

<span class="c"># 生成随机数: D = (input_values, target_class)</span>
<span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">feats</span><span class="p">),</span> <span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

</code></pre></div></div>

<h4 id="建立模型">建立模型</h4>

<p>接下来，定义神经网络。</p>

<p>先定义一个大的图片，编辑好图片的小部件，再把训练数据集放到图片中去自动地训练。</p>

<p>定义 x 和 y，相当于 placeholder。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 定义 x y 容器</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">"x"</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dvector</span><span class="p">(</span><span class="s">"y"</span><span class="p">)</span>
</code></pre></div></div>

<p>初始化 weights 和 bias。 有多少 features 就生成多少个 weights， 今天只是用最基本的 input 和 output 层的神经网络，如果想用 hidden layer 可以参考上一节课的例子。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 初始化 weights and biases</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">feats</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">"w"</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"b"</span><span class="p">)</span>
</code></pre></div></div>

<p>定义激活函数，交叉熵。 p_1 是用 sigmoid 求的概率，输入越小，则概率值越接近 0，越大则越接近 1，等于 0 则值为 0.5. p_1 &gt; 0.5 时，预测值为 True，即为 1。 然后计算针对每个 sample 的交叉熵 xent。 再计算整批数据的 cost，为了减小 overfitting，这里加入了 L1-正则化。 接下来可以计算 weights 和 bias 的梯度 gW, gb。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p_1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>   <span class="c"># sigmoid 激励函数</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">p_1</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="n">xent</span> <span class="o">=</span> <span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_1</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p_1</span><span class="p">)</span> <span class="c"># 交叉熵</span>

<span class="c"># xent 也可以使用下面这个达到一样的效果</span>
<span class="c"># xent = T.nnet.binary_crossentropy(p_1, y) </span>

<span class="n">cost</span> <span class="o">=</span> <span class="n">xent</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">W</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>  <span class="c"># l2 正则化</span>
<span class="n">gW</span><span class="p">,</span> <span class="n">gb</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="激活网络">激活网络。</h4>

<p>学习率需要小于 1. 接下来定义两个函数 train 和 predict，方法和上一节课的类似。 outputs 可以输出两个 prediction 和交叉熵损失的平均值 xent.mean。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
          <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span>
          <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">prediction</span><span class="p">,</span> <span class="n">xent</span><span class="o">.</span><span class="n">mean</span><span class="p">()],</span>
          <span class="n">updates</span><span class="o">=</span><span class="p">((</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gW</span><span class="p">),</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gb</span><span class="p">)))</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">prediction</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="训练模型">训练模型</h4>

<p>接下来训练模型。 用训练集的 feature 和 target 训练模型，输出预测值和损失 pred, err。 每 50 步打印一次损失和准确率。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c"># Training</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">pred</span><span class="p">,</span> <span class="n">err</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">D</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">D</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'cost:'</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"accuracy:"</span><span class="p">,</span> <span class="n">compute_accuracy</span><span class="p">(</span><span class="n">D</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">predict</span><span class="p">(</span><span class="n">D</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>

</code></pre></div></div>

<p>最后打印出预测值与实际值进行比较。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"target values for D:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">D</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"prediction on D:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">D</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="s">"""
cost: 11.677533008109728
accuracy: 0.52
cost: 6.1946164642562636
accuracy: 0.6175
cost: 3.012375762498935
accuracy: 0.725
cost: 1.3340537876600198
accuracy: 0.8275
cost: 0.4690120202455575
accuracy: 0.9075
...


target values for D:
[1 1 0 1 0 1 0 1 1 1 1 1 .....]

prediction on D:
[1 1 0 1 0 1 0 1 1 1 1 1 .....]
"""</span>
</code></pre></div></div>

